{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy-d-Uz9U8NU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AdamW, set_seed, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSequenceClassification,BertTokenizer,BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBdNowjsH23G"
      },
      "outputs": [],
      "source": [
        "def read_data(data_path):\n",
        "    data = pd.read_csv(data_path)\n",
        "    data_train, data_test = train_test_split(data, test_size=0.3, random_state=42)\n",
        "    return data_train, data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z448B0DmVKt6"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(tokenizer, data):\n",
        "    X = data.drop([\"Label\"], axis=1)\n",
        "    y = data[\"Label\"]\n",
        "\n",
        "    encode = tokenizer.batch_encode_plus(\n",
        "        X[\"Tweet\"].tolist(),\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=150,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = encode[\"input_ids\"]\n",
        "    attention_masks = encode[\"attention_mask\"]\n",
        "    labels = torch.tensor(y.tolist())\n",
        "\n",
        "    if \"token_type_ids\" in encode:\n",
        "        token_type_ids = encode[\"token_type_ids\"]\n",
        "        dataset = TensorDataset(input_ids, token_type_ids, attention_masks, labels)\n",
        "    else:\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6YYr83XVbvZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training loops with case checking whether there is token_type_ids or not\n",
        "\"\"\"\n",
        "\n",
        "def train_model(model, dataloader_train, epochs, device):\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_acc = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(dataloader_train):\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[-1].to(device)\n",
        "\n",
        "\n",
        "            if len(batch) == 4:  # Token type IDs are included\n",
        "                token_type_ids = batch[2].to(device)\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels)\n",
        "            else:\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "\n",
        "\n",
        "            loss = outputs.loss\n",
        "            acc = (torch.log_softmax(outputs.logits, dim=1).argmax(dim=1) == labels).sum().float() / float(labels.size(0))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            total_train_acc += acc.item()\n",
        "\n",
        "        train_acc = total_train_acc / len(dataloader_train)\n",
        "        train_loss = total_train_loss / len(dataloader_train)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WNquInhWb-2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Evalaution loops with case checking whether there is token_type_ids or not \"\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_model(model, dataloader_test, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    test_gold = []\n",
        "    test_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader_test:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[-1].to(device)\n",
        "\n",
        "            if len(batch) == 4:  # Token type IDs are included\n",
        "                token_type_ids = batch[2].to(device)\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels)\n",
        "            else:\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            preds = torch.log_softmax(outputs.logits, dim=1).argmax(dim=1)\n",
        "            test_gold.extend(labels.tolist())\n",
        "            test_pred.extend(preds.tolist())\n",
        "    print(classification_report(test_gold, test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baS4vkkoWiX1"
      },
      "outputs": [],
      "source": [
        "# Set the seed for reproducibility\n",
        "set_seed(36)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fkUo4TyXsy5"
      },
      "outputs": [],
      "source": [
        "data_train, data_test = read_data(\"covid19-tweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stro0Sj7WmdL"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer for each model\n",
        "tokenizer_roberta = AutoTokenizer.from_pretrained(\"amitness/roberta-base-ne\")\n",
        "\n",
        "tokenizer_distilbert = AutoTokenizer.from_pretrained(\"Sakonii/distilbert-base-nepali\")\n",
        "\n",
        "tokenizer_nepalibert = AutoTokenizer.from_pretrained(\"Rajan/NepaliBERT\")\n",
        "\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0X9rzdlZfif",
        "outputId": "3565c945-4035-490a-d76c-59160d66a5b8"
      },
      "outputs": [],
      "source": [
        "# Load each of the models\n",
        "\n",
        "model_roberta = AutoModelForSequenceClassification.from_pretrained(\"amitness/roberta-base-ne\", num_labels=3)\n",
        "\n",
        "model_distillbert = AutoModelForSequenceClassification.from_pretrained(\"Sakonii/distilbert-base-nepali\", num_labels=3)\n",
        "\n",
        "model_nepalibert = AutoModelForSequenceClassification.from_pretrained(\"Rajan/NepaliBERT\", num_labels=3)\n",
        "\n",
        "model_bert = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-uncased\",num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tO5njzWWorK"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data for each tokenizer\n",
        "dataset_train_roberta = preprocess_data(tokenizer_roberta, data_train)\n",
        "dataset_test_roberta = preprocess_data(tokenizer_roberta, data_test)\n",
        "\n",
        "dataset_train_distillbert = preprocess_data(tokenizer_distilbert, data_train)\n",
        "dataset_test_distillbert = preprocess_data(tokenizer_distilbert, data_test)\n",
        "\n",
        "dataset_train_nepalibert = preprocess_data(tokenizer_nepalibert, data_train)\n",
        "dataset_test_nepalibert = preprocess_data(tokenizer_nepalibert, data_test)\n",
        "\n",
        "dataset_train_bert = preprocess_data(tokenizer_bert, data_train)\n",
        "dataset_test_bert = preprocess_data(tokenizer_bert, data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sAUSEnoWvC7"
      },
      "outputs": [],
      "source": [
        "# Create the data loaders\n",
        "batch_size = 16\n",
        "dataloader_train_roberta = DataLoader(dataset_train_roberta, sampler=RandomSampler(dataset_train_roberta), batch_size=batch_size)\n",
        "dataloader_test_roberta = DataLoader(dataset_test_roberta, sampler=RandomSampler(dataset_test_roberta), batch_size=batch_size)\n",
        "\n",
        "dataloader_train_distillbert = DataLoader(dataset_train_distillbert, sampler=RandomSampler(dataset_train_distillbert), batch_size=batch_size)\n",
        "dataloader_test_distillbert = DataLoader(dataset_test_distillbert, sampler=RandomSampler(dataset_test_distillbert), batch_size=batch_size)\n",
        "\n",
        "dataloader_train_nepalibert = DataLoader(dataset_train_nepalibert, sampler=RandomSampler(dataset_train_nepalibert), batch_size=batch_size)\n",
        "dataloader_test_nepalibert = DataLoader(dataset_test_nepalibert, sampler=RandomSampler(dataset_test_nepalibert), batch_size=batch_size)\n",
        "\n",
        "dataloader_train_bert = DataLoader(dataset_train_bert, sampler=RandomSampler(dataset_train_bert), batch_size=batch_size)\n",
        "dataloader_test_bert = DataLoader(dataset_test_bert, sampler=RandomSampler(dataset_test_bert), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icwEk4pMYBbT",
        "outputId": "8b6e45bb-45af-4249-a2ff-b2dbaaae697c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSmS_HJvYBgG",
        "outputId": "53033f4e-5782-4725-aa6e-3381ebf5bf85"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training  for amitness/roberta-base-ne\n",
        "\"\"\"\n",
        "print(\"Training for amitness/roberta-base-ne\")\n",
        "model_roberta.to(device)\n",
        "train_model(model_roberta, dataloader_train_roberta, epochs=10, device=device)\n",
        "\n",
        "\"\"\"\n",
        "Evaluation for amitness/roberta-base-ne\n",
        "\"\"\"\n",
        "print(\"Evaluation for amitness/roberta-base-ne\")\n",
        "evaluate_model(model_roberta, dataloader_test_roberta, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV2xlnVnYBil",
        "outputId": "d4fe3a6d-2baf-4424-ffe8-c4db85ce5d48"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training for Sakonii/distilbert-base-nepali\n",
        "\"\"\"\n",
        "print(\"Training for Sakonii/distilbert-base-nepali\")\n",
        "model_distillbert.to(device)\n",
        "train_model(model_distillbert, dataloader_train_distillbert, epochs=10, device=device)\n",
        "\n",
        "\"\"\"\n",
        "Evaluation for Sakonii/distilbert-base-nepali\n",
        "\"\"\"\n",
        "print(\"Evaluation for Sakonii/distilbert-base-nepali\")\n",
        "evaluate_model(model_distillbert, dataloader_test_distillbert, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnLupRCKYOc3",
        "outputId": "df0d8bb6-9c68-4b33-c014-1416dfc2a0a3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training for Rajan/NepaliBERT\n",
        "\"\"\"\n",
        "print(\"Training for Rajan/NepaliBERT\")\n",
        "model_nepalibert.to(device)\n",
        "train_model(model_nepalibert, dataloader_train_nepalibert, epochs=10, device=device)\n",
        "\n",
        "\"\"\"\n",
        "Evaluation for Rajan/NepaliBERT\n",
        "\"\"\"\n",
        "print(\"Evaluation for Rajan/NepaliBERT\")\n",
        "evaluate_model(model_nepalibert, dataloader_test_nepalibert, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UrB5IkAZxzL",
        "outputId": "f72a4550-a624-4969-e1cc-f0d859f6dfdd"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training for bert-base-multilingual-uncased\n",
        "\"\"\"\n",
        "print(\"Training for bert-base-multilingual-uncased\")\n",
        "model_bert.to(device)\n",
        "train_model(model_bert, dataloader_train_bert, epochs=10, device=device)\n",
        "\n",
        "\"\"\"\n",
        "Evaluation for bert-base-multilingual-uncased\n",
        "\"\"\"\n",
        "print(\"Evaluation for bert-base-multilingual-uncased\")\n",
        "evaluate_model(model_bert, dataloader_test_bert, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_g5Oo609Flu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
